# Today I Learned - 2022/05/17 Tue

# Naver finance daily Web scraping
`table`  íƒœê·¸ê°€ ë“¤ì–´ê°ì—ë„ ë¶ˆêµ¬í•˜ê³  `read_html` ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì„ ë•Œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì´ë‹¤.

## ë¼ì´ë¸ŒëŸ¬ë¦¬
```python
import pandas as pd
import numpy as np
import requests
```

## ìˆ˜ì§‘í•  URL ì •í•˜ê¸°
```
item_code = "323410"
item_name = "ì¹´ì¹´ì˜¤ë±…í¬

item_code = "005930"
item_name = "ì‚¼ì„±ì „ì"
```

```python
# ì¢…ëª©ë²ˆí˜¸ì™€ ìƒì¥ì‚¬ ì´ë¦„ì„ item_codeì™€ item_nameìœ¼ë¡œ ì„¤ì •

item_code = "005930"
item_name = "ì‚¼ì„±ì „ì"

url = f"https://finance.naver.com/item/sise_day.naver?code={item_code}&page=1"
print(url)
```

`[https://finance.naver.com/item/sise_day.naver?code=005930&page=1](https://finance.naver.com/item/sise_day.naver?code=005930&page=1)`

## Pandas read_htmlë¡œ ë¶ˆëŸ¬ì˜¤ê¸°
table íƒœê·¸ë¡œ ë˜ì–´ìˆìŒì—ë„ ë¶ˆêµ¬í•˜ê³  ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí•œë‹¤.

```python
table = pd.read_html(url, encoding="cp949")

print(len(table))
table[0]
```

### ğŸ“ euc-krê³¼ cp949ì˜ ì°¨ì´ì 

|  | euc-kr | cp949 |
| --- | --- | --- |
| ì°¨ì´ì  | 2350ì | 11172ì |

## requestsë¥¼ í†µí•œ HTTP ìš”ì²­
`Request Method` ì— `GET` ì´ë¼ê³  ë˜ì–´ ìˆìœ¼ë¯€ë¡œ `requests.get` ì„ ì´ìš©í•˜ì—¬ ê°€ì ¸ì˜¨ë‹¤.

ë¡œë´‡ì¸ì§€ ì•„ë‹Œì§€ë¥¼ íŒë³„í•  ìˆ˜ ìˆë„ë¡ USERì˜ Agent ì •ë³´ë¥¼ Headerì— ì…ë ¥í•´ì£¼ì–´ì•¼ í•œë‹¤.

```python
headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.67 Safari/537.36'}

requests.get(url)  # <Response [200]> ë¼ê³  ë‚˜íƒ€ë‚˜ë©´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•œ ê²ƒì´ë‹¤.

response = requests.get(url, headers=headers)
response.status_code  # 200ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ë©´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•œ ê²ƒì´ë‹¤.

response.text
```

```python
print(headers, type(headers), headers["user-agent"])
# {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.67 Safari/537.36'} <class 'dict'> Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.67 Safari/537.36
```

### ğŸ“ HTTP ìƒíƒœ ì½”ë“œ

| ì½”ë“œ | ìƒíƒœ | ì„¤ëª… |
| --- | --- | --- |
| 100 | ì •ë³´ | ìš”ì²­ì„ ë°›ì•˜ìœ¼ë©° í”„ë¡œì„¸ìŠ¤ë¥¼ ê³„ì†í•œë‹¤. |
| 200 | ì„±ê³µ | ìš”ì²­ì„ ì„±ê³µì ìœ¼ë¡œ ë°›ì•˜ìœ¼ë©° ìˆ˜ìš©í–ˆë‹¤. |
| 300 | ë¦¬ë‹¤ì´ë ‰ì…˜ | ìš”ì²­ ì™„ë£Œë¥¼ ìœ„í•´ ì¶”ê°€ ì‘ì—… ì¡°ì¹˜ê°€ í•„ìš”í•˜ë‹¤. |
| 400 | í´ë¼ì´ì–¸íŠ¸ ì˜¤ë¥˜ | ì˜¤ë¥˜ ìš”ì²­ì˜ ë¬¸ë²•ì´ ì˜ëª»ë˜ì—ˆê±°ë‚˜ ìš”ì²­ì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ë‹¤. |
| 500 | ì„œë²„ ì˜¤ë¥˜ | ì„œë²„ê°€ ëª…ë°±íˆ ìœ íš¨í•œ ìš”ì²­ì— ëŒ€í•´ ì‹¤íŒ¨í–ˆë‹¤. |

## BeautifulSoup ì„ í†µí•œ table íƒœê·¸ ì°¾ê¸°

`lxml` ì´ë‚˜ `html` ì„ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ëª¨ë“ˆì´ë‹¤.

`response.text` ì— ë¹„í•´ í›¨ì”¬ ê¹”ë”í•˜ê³  ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥ëœë‹¤.

<ì°¸ê³  ë¬¸ì„œ>

[https://www.crummy.com/software/BeautifulSoup/bs4/doc/](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)

```python
from bs4 import BeautifulSoup as bs

soup = bs(response.text, "lxml")
soup
```

```python
soup.title  # <title>ë„¤ì´ë²„ ê¸ˆìœµ</title>
soup.find_all('a')
soup.find_all('table')
soup.table  # table íƒœê·¸ ì°¾ê¸°
```

- ì£¼ì˜ â— `list` ë¡œëŠ” ì½ì–´ë“¤ì¼ ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— `str` í˜•ì‹ìœ¼ë¡œ ë°”ê¿”ì„œ ì‚¬ìš©í•œë‹¤.

```python
tables = soup.select("table")  # find_allê³¼ ë™ì¼í•œ ê²°ê³¼ê°€ ë‚˜íƒ€ë‚œë‹¤.

table = pd.read_html(str(tables), encoding="cp949")

print(len(table))
table[0]
```

### requests â†’ BeautifulSoup â†’ pd.read_html ê³¼ì • ì´ìœ â“

<aside>
ğŸ’¡ ì •ìƒì ì¸ ì ‘ê·¼ì¸ì§€ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ì„œì´ë‹¤.

</aside>

### ğŸ“ íŒŒì„œì˜ ì‚¬ìš©ë²• ë° ì¥ë‹¨ì 

| íŒŒì„œ | ì‚¬ìš©ë²” | ì¥ì  ë° ë‹¨ì  |
| --- | --- | --- |
| html.parser | BeautifulSoup(markup, "html.parser") | lxml ë³´ë‹¤ëŠ” ë¹ ë¥´ì§€ ì•Šì§€ë§Œ html5libë³´ë‹¤ëŠ” ë¹ ë¥´ë‹¤. |
| lxml | BeautifulSoup(markup, "lxml") | ë§¤ìš° ë¹ ë¥´ì§€ë§Œ í—ˆìˆ í•˜ë‹¤ |
| lxml-xml | BeautifulSoup(markup, "lxml-xml") | ë§¤ìš° ë¹ ë¥´ë‹¤ |
| html5lib | BeautifulSoup(markup, "html5lib") | ë§¤ìš° ê´€ëŒ€í•˜ë‹¤ |

```python
temp = table[0].dropna(how="all", axis=0)  # ê²°ì¸¡ì¹˜ ì œê±°
temp
```

## í˜ì´ì§€ë³„ ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜ ë§Œë“¤ê¸°

```python
def get_day_list(item_code, page_no):
    
    url = f"https://finance.naver.com/item/sise_day.naver?code={item_code}&page={page_no}"
    headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.67 Safari/537.36'}

    response = requests.get(url, headers=headers)
    soup = bs(response.text, "lxml")

    tables = soup.select("table")
    table = pd.read_html(str(tables), encoding="cp949")

    df = table[0].dropna(how="all", axis=0)
    
    return df
```

```python
item_code = "005930"
item_name = "ì‚¼ì„±ì „ì"
page_no = 1

get_day_list(item_code, page_no)
```

## ë°˜ë³µë¬¸ì„ í†µí•œ ì „ì²´ ì¼ì ë°ì´í„° ìˆ˜ì§‘í•˜ê¸°

### 1. forë¬¸

```python
import time
from tqdm import trange

page_no = 1
item_list = []

for page in trange(page_no, 11):
    page = get_day_list(item_code, page)
    item_list.append(page)
    time.sleep(0.5)

df = pd.concat(item_list, axis=0)
df.head()
```

### 2. while

```python
page_no = 1
item_list = []

last_page = soup.find_all('a')[-1]["href"].split("=")[-1]  # 14

while True:
    page = get_day_list(item_code, page_no)
    item_list.append(page)
    time.sleep(0.5)
    
    page_no += 1
    
    if page_no == int(last_page) + 1:
        break
```

### 3. while

```python
page_no = 1
item_list = []

item_code = "377300"
item_name = "ì¹´ì¹´ì˜¤í˜ì´"

prev = ""

while True:
    one_page = get_day_list(item_code, page_no)
    curr = one_page.iloc[-1, 0]  # ë§ˆì§€ë§‰ ë‚ ì§œ
    
    # ë§ˆì§€ë§‰ ë‚ ì§œë¥¼ ë¹„êµ
    if curr == prev:
        break
        
    item_list.append(one_page)
    page_no += 1
    
    prev = curr  # í˜„ì¬ ë‚ ì§œë¥¼ ì´ì „ ë‚ ì§œ ë³€ìˆ˜ì— ë‹´ì•„ì„œ ë‹¤ìŒ ë²ˆì— ë¹„êµí•œë‹¤.
```

- ìˆ˜ì§‘í•œ ë°ì´í„° í•˜ë‚˜ì˜ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ í•©ì¹˜ê¸°
- ë°ì´í„°í”„ë ˆì„ì— ì¢…ëª©ì½”ë“œì™€ ì¢…ëª©ëª… ì¶”ê°€
- ì»¬ëŸ¼ ìˆœì„œ ë³€ê²½
- ì¤‘ë³µ ë°ì´í„° ì œê±°
- ìµœê·¼ ë‚ ì§œ êµ¬í•´ì„œ íŒŒì¼ëª… ë§Œë“¤ê¸°

```python
df = pd.concat(item_list, axis=0)

df["ì¢…ëª©ì½”ë“œ"] = item_code
df["ì¢…ëª©ëª…"] = item_name

cols = ['ì¢…ëª©ì½”ë“œ', 'ì¢…ëª©ëª…', 'ë‚ ì§œ', 'ì¢…ê°€', 'ì „ì¼ë¹„', 'ì‹œê°€', 'ê³ ê°€', 'ì €ê°€', 'ê±°ë˜ëŸ‰']
df = df[cols]

df = df.drop_duplicates()
df = df.drop_duplicates("ë‚ ì§œ")  # ë‚ ì§œê°€ ê°™ì€ ì¤‘ë³µ ê°’ì„ ì œê±°

date = max(df["ë‚ ì§œ"])

file_name = f"{item_name}_{item_code}_{date}"  # 'ì¹´ì¹´ì˜¤í˜ì´_377300_2022.05.17'

df.to_csv(file_name, index=False)
pd.read_csv(file_name)
```
